{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b067c9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30f3519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"create_merchant_recommendation_dataMart\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b0ec576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Config Updated for maximum stability.\n"
     ]
    }
   ],
   "source": [
    "# CRITICAL FIXES: Must be run BEFORE loading data to prevent ClassCastException\n",
    "# 1. Disable Vectorized Reader (Avoids low-level ORC data reading crash)\n",
    "spark.conf.set(\"spark.sql.orc.enableVectorizedReader\", \"false\")\n",
    "spark.conf.set(\"spark.sql.hive.convertMetastoreOrc\", \"false\")\n",
    "# 2. Disable Broadcast Join (Avoids memory/shuffle crash on join)\n",
    "#spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1) \n",
    "# 3. Disable Spark's optimizing components (Forces safer execution path)\n",
    "#spark.conf.set(\"spark.sql.cbo.enabled\", \"false\") \n",
    "#spark.conf.set(\"spark.sql.codegen.wholeStage\", \"false\")\n",
    "# 4. Force Hive SerDe (Ultimate attempt to bypass native Spark reader)\n",
    "spark.conf.set(\"spark.sql.hive.convertMetastore\", \"false\") \n",
    "\n",
    "print(\"Spark Config Updated for maximum stability.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b642c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F \n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType, DecimalType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909d038f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Load the tables directly from the Database\n",
    "#We use the standard \"database_name.table_name\" format\n",
    "\n",
    "print(\"Loading Silver Layer table from Hive..\")\n",
    "\n",
    "#A. Transaction Table\n",
    "df_trans=spark.table(\"financial_db.transactions_silver\")\n",
    "\n",
    "#B. Users Table\n",
    "df_users=spark.table(\"financial_db.users_silver\")\n",
    "\n",
    "#C.Cards Table\n",
    "df_cards=spark.table(\"financial_db.cards_silver\")\n",
    "\n",
    "#3.Verification\n",
    "print(f\"Transaction Count:{df_trans.count()}\")\n",
    "print(f\"Users Count: {df_users.count()}\")\n",
    "print(f\"Cards Count: {df_cards.count()}\")\n",
    "\n",
    "#4.Preview Schema to ensure types are correct\n",
    "df_trans.printSchema()\n",
    "df_users.printSchema()\n",
    "df_cards.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf883d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc10ba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----2.Fixing Users Table----\")\n",
    "\n",
    "#1. Generate 'user_id' using Row Number(0,1,2)\n",
    "rdd_with_id=df_users.rdd.zipWithIndex().map(lambda x:(x[1],)+tuple(x[0]))\n",
    "\n",
    "#2.Create DataFrame with \"user_id\" as the first column \n",
    "new_column_names=[\"user_id\"]+df_users.columns\n",
    "\n",
    "df_users_indexed=spark.createDataFrame(rdd_with_id,new_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aa9620",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_users_indexed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b349fa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cards.select(\"card_number\",\"card_type\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8972eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- 1. Prep Users (Golden Layer Standardization) ---\")\n",
    "# Preparing Users table with standardized column names and clean datatypes\n",
    "df_users_prep = df_users.select(\n",
    "    # Primary user identifier (cast to INT for join compatibility)\n",
    "    F.col(\"person_id\").cast(\"int\").alias(\"user_id\"),\n",
    "\n",
    "    # Demographic attributes\n",
    "    F.col(\"current_age\").alias(\"age\"),\n",
    "    F.col(\"gender\"),\n",
    "\n",
    "    # Location attributes\n",
    "    F.col(\"city\"),\n",
    "    F.col(\"state\"),\n",
    "    F.col(\"zipcode\").alias(\"user_zip\"),\n",
    "\n",
    "    # Income & financial attributes\n",
    "    F.coalesce(F.col(\"per_capita_income_zipcode\").cast(\"double\"), F.lit(0.0)).alias(\"per_capita_income\"),\n",
    "    F.coalesce(F.col(\"yearly_income_person\").cast(\"double\"), F.lit(0.0)).alias(\"yearly_income\"),\n",
    "    F.coalesce(F.col(\"total_debt\").cast(\"double\"), F.lit(0.0)).alias(\"total_debt\"),\n",
    "\n",
    "    # Credit profile attributes\n",
    "    F.col(\"fico_score\"),\n",
    "    F.col(\"num_credit_cards\")\n",
    ")\n",
    "\n",
    "print(\"Users Prepped (Schema Updated).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d92698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- 2. Prep Transactions (Golden Layer Standardization) ---\")\n",
    "# Preparing Transactions table with cleaned columns\n",
    "df_trans_prep = df_trans.select(\n",
    "    # Primary join keys\n",
    "    F.col(\"user_id\"),     \n",
    "    F.col(\"card_id\"),     \n",
    "\n",
    "    # Transaction timestamp\n",
    "    F.col(\"transaction_timestamp\"),\n",
    "\n",
    "    # Merchant attributes\n",
    "    F.col(\"merchant_name\"),\n",
    "    F.col(\"merchant_state\"),\n",
    "    F.col(\"zip\").alias(\"merchant_zip\"),\n",
    "\n",
    "    # Merchant category code (MCC)\n",
    "    F.col(\"mcc\").alias(\"merchant_category\"), \n",
    "\n",
    "    # Error and chip usage indicators\n",
    "    F.col(\"errors\").alias(\"error_code\"), \n",
    "    F.col(\"use_chip\"),\n",
    "\n",
    "    # Transaction amount (null-safe & numeric)\n",
    "    F.coalesce(F.col(\"amount\").cast(\"double\"), F.lit(0.0)).alias(\"amount\"),\n",
    "\n",
    "    # Fraud label (binary for ML)\n",
    "    F.when(F.col(\"is_fraud\") == \"Yes\", 1).otherwise(0).alias(\"label_is_fraud\")\n",
    "\n",
    ").withColumn(\n",
    "    # Extract hour of transaction for behavioral patterns\n",
    "    \"hour_of_day\",\n",
    "    F.hour(F.col(\"transaction_timestamp\"))\n",
    ")\n",
    "\n",
    "print(\"Transactions Prepped (Schema Updated).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67665e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- 3. Prep Cards (Golden Layer Standardization) ---\")\n",
    "# Prepare Cards table with clean identifiers and financial attributes\n",
    "df_cards_prep = df_cards.select(\n",
    "    # Primary join keys\n",
    "    F.col(\"user\").alias(\"user_id\"),\n",
    "    F.col(\"card_index\").alias(\"card_id\"),\n",
    "\n",
    "    # Card attributes\n",
    "    F.col(\"card_brand\"),\n",
    "    F.col(\"card_type\"),\n",
    "\n",
    "    # Financial capacity of the card\n",
    "    F.coalesce(F.col(\"credit_limit\").cast(\"double\"), F.lit(0.0)).alias(\"credit_limit\"),\n",
    "\n",
    "    # Security & risk attributes\n",
    "    F.col(\"has_chip\").alias(\"card_has_chip\"),\n",
    "    F.col(\"card_on_dark_web\").alias(\"dark_web_exposure\"),\n",
    "\n",
    "    # Account lifecycle attributes\n",
    "    F.col(\"acct_opened_year\"),\n",
    "    F.col(\"year_pin_last_changed\")\n",
    ")\n",
    "\n",
    "print(\"Cards Prepped (Schema Updated).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaae46e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- 4. Join Transactions with Users ---\")\n",
    "from pyspark.sql.functions import col, trim\n",
    "\n",
    "# Transactions: user_id as string\n",
    "df_trans_prep = df_trans_silver \\\n",
    "    .withColumn(\"user_id_str\", col(\"user_id\").cast(\"string\"))\n",
    "\n",
    "# Users: person_id cleaned\n",
    "df_users_prep = df_users_silver \\\n",
    "    .withColumn(\"user_id_str\", trim(col(\"person_id\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f887c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- 5. Join Cards with Transactions + Users ---\")\n",
    "df_golden = df_tx_user.join(\n",
    "    df_cards_prep,\n",
    "    on=[\"user_id\", \"card_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"Cards Joined. Golden Base Dataset Created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eecc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- 6. Feature Engineering for Spend Analysis ---\")\n",
    "\n",
    "CURRENT_YEAR = 2026\n",
    "\n",
    "df_golden = (\n",
    "    df_golden\n",
    "    # Flag indicating whether chip was used (binary behavioral feature)\n",
    "    .withColumn(\n",
    "        \"is_chip_txn\",\n",
    "        F.when(F.col(\"use_chip\") == \"Yes\", 1).otherwise(0)\n",
    "    )\n",
    "\n",
    "    # Flag for cross-state transactions (travel / anomaly indicator)\n",
    "    .withColumn(\n",
    "        \"is_out_of_state_txn\",\n",
    "        F.when(F.col(\"merchant_state\") != F.col(\"state\"), 1).otherwise(0)\n",
    "    )\n",
    "\n",
    "    # Ratio of debt to income (financial stress indicator)\n",
    "    .withColumn(\n",
    "        \"debt_to_income_ratio\",\n",
    "        F.when(\n",
    "            F.col(\"yearly_income\") > 0,\n",
    "            F.col(\"total_debt\") / F.col(\"yearly_income\")\n",
    "        ).otherwise(0.0)\n",
    "    )\n",
    "\n",
    "    # Spend to income ratio (affordability & overspend detection)\n",
    "    .withColumn(\n",
    "        \"spend_to_income_ratio\",\n",
    "        F.when(\n",
    "            F.col(\"yearly_income\") > 0,\n",
    "            F.col(\"amount\") / F.col(\"yearly_income\")\n",
    "        ).otherwise(0.0)\n",
    "    )\n",
    "\n",
    "    # Credit utilization proxy (single-txn utilization signal)\n",
    "    .withColumn(\n",
    "        \"credit_utilization_ratio\",\n",
    "        F.when(\n",
    "            F.col(\"credit_limit\") > 0,\n",
    "            F.col(\"amount\") / F.col(\"credit_limit\")\n",
    "        ).otherwise(0.0)\n",
    "    )\n",
    "\n",
    "    # Card account age in years (tenure & trust indicator)\n",
    "    .withColumn(\n",
    "        \"card_account_age_years\",\n",
    "        F.when(\n",
    "            F.col(\"acct_opened_year\").isNotNull(),\n",
    "            F.lit(CURRENT_YEAR) - F.col(\"acct_opened_year\")\n",
    "        ).otherwise(None)\n",
    "    )\n",
    "\n",
    "    # PIN age in years (security hygiene indicator)\n",
    "    .withColumn(\n",
    "        \"pin_age_years\",\n",
    "        F.when(\n",
    "            F.col(\"year_pin_last_changed\").isNotNull(),\n",
    "            F.lit(CURRENT_YEAR) - F.col(\"year_pin_last_changed\")\n",
    "        ).otherwise(None)\n",
    "    )\n",
    "\n",
    "    # High value transaction flag based on credit limit threshold\n",
    "    .withColumn(\n",
    "        \"is_high_value_txn\",\n",
    "        F.when(\n",
    "            (F.col(\"credit_limit\") > 0) &\n",
    "            (F.col(\"amount\") > (0.2 * F.col(\"credit_limit\"))),\n",
    "            1\n",
    "        ).otherwise(0)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Feature Engineering Completed for Spend Analysis.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af54b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- 7. Final Validation ---\")\n",
    "\n",
    "# Print schema to validate final Golden Layer structure\n",
    "df_golden.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bce8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Step 6: Saving as External Hive Table ---\")\n",
    "# Path for the External Table\n",
    "gold_path = \"/user/talentum/projectMaster/warehouseDir/gold/spend_analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316eb962",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.orc.enableVectorizedReader\", \"false\")\n",
    "spark.conf.set(\"spark.sql.hive.convertMetastoreOrc\", \"false\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854024e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS financial_db.spend_analysis_gold\")\n",
    "\n",
    "# Write with explicit partitioning and Hive serde override\n",
    "df_golden.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"orc\") \\\n",
    "    .option(\"path\", \"/user/talentum/projectMaster/warehouseDir/gold/spend_analysis\") \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .saveAsTable(\"financial_db.spend_analysis_gold\")\n",
    "\n",
    "print(\"SUCCESS: Table saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab57c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show sample records to visually inspect joins and engineered features\n",
    "df_golden.show(10, truncate=False)\n",
    "\n",
    "print(\"Golden Layer Spend Analysis Dataset Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1411ce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Step 6: Saving as External Hive Table ---\")\n",
    "# Path for the External Table\n",
    "gold_path = \"/user/talentum/projectMaster/warehouseDir/gold/spend_analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589cc5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.orc.enableVectorizedReader\", \"false\")\n",
    "spark.conf.set(\"spark.sql.hive.convertMetastoreOrc\", \"false\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae9fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS financial_db.spend_analysis_gold\")\n",
    "\n",
    "# Write with explicit partitioning and Hive serde override\n",
    "df_golden.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"orc\") \\\n",
    "    .option(\"path\", \"/user/talentum/projectMaster/warehouseDir/gold/spend_analysis\") \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .saveAsTable(\"financial_db.spend_analysis_gold\")\n",
    "\n",
    "print(\"SUCCESS: Table saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a76fafe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
