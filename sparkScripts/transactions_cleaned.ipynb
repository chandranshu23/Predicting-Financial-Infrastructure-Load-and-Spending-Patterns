{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5d26a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dae381c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Transaction Data Cleaning\").enableHiveSupport().getOrCreate()\n",
    "spark.conf.set(\"spark.sql.parquet.outputTimestampType\", \"TIMESTAMP_MILLIS\")\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a26de1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import DecimalType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88f7ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the parquet file from hdfs to perform data cleaning\n",
    "df = spark.read.orc('/user/talentum/projectMaster/dataStaging/credit_card_transactions-ibm_v2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7508ac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing '$' from the amount column and converting it to float data type\n",
    "df_cleaning = df.withColumn('Amount', F.regexp_replace(F.col('Amount'), '[$,]', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92987709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Casting the Amount cloumn to Float(10,2)\n",
    "df_cleaned = df_cleaning.withColumn(\n",
    "    'Amount',\n",
    "    F.col('Amount').cast(DecimalType(10, 2))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "354daaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing word Transaction from the field Use_Chip\n",
    "df_cleaning = df_cleaning.withColumn('Use_Chip', F.regexp_replace(F.col('Use_Chip'), ' Transaction', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9255bf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing null with a placeholder 'N/A'\n",
    "df_cleaning = df_cleaning.fillna('N/A', subset=['Errors?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6485fbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Casting Zip to string type\n",
    "df_cleaning = df_cleaning.withColumn(\"Zip\", \n",
    "    F.coalesce(\n",
    "        # Robust conversion: Cast float to int to safely remove .0, then pad to 5 digits\n",
    "        F.lpad(F.col(\"Zip\").cast(\"float\").cast(\"int\").cast(\"string\"), 5, \"0\"), \n",
    "        F.lit(\"00000\") # Fill Null/Missing Zip codes with '00000'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c2b3020",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a Date_str column to also have consolidated dates\n",
    "df_cleaning = df_cleaning.withColumn(\n",
    "    # Create the date string: YYYY-MM-DD\n",
    "    \"Date_Str\",\n",
    "    F.concat_ws(\"-\",\n",
    "        F.col(\"Year\"),\n",
    "        # Pad Month and Day to ensure 2 digits (e.g., 8 -> 08) for standard format\n",
    "        F.lpad(F.col(\"Month\").cast(\"string\"), 2, \"0\"),\n",
    "        F.lpad(F.col(\"Day\").cast(\"string\"), 2, \"0\")\n",
    "    )\n",
    ").withColumn(\n",
    "    # Combine Date_Str and Time into a single TIMESTAMP object\n",
    "    \"Transaction_Timestamp\",\n",
    "    F.to_timestamp(F.concat(F.col(\"Date_Str\"), F.lit(\" \"), F.col(\"Time\")), \"yyyy-MM-dd HH:mm\")\n",
    ").drop(\"Day\", \"Time\", \"Date_Str\") # Drop original raw columns\n",
    "\n",
    "#Converting merchant name to string from long\n",
    "df_cleaning = df_cleaning.withColumn(\n",
    "    \"Merchant_Name\", \n",
    "    F.col(\"Merchant_Name\").cast(\"string\")\n",
    ")\n",
    "\n",
    "# Iterate through every column in the DataFrame\n",
    "for old_name in df_cleaning.columns:\n",
    "    \n",
    "    # 1. Clean the name (removes the tricky '?' from Errors? and Is_Fraud?)\n",
    "    cleaned_name = old_name.replace('?', '')\n",
    "    \n",
    "    # 2. Lowercase the entire name (e.g., 'User' -> 'user', 'Merchant_Name' -> 'merchant_name')\n",
    "    new_name = cleaned_name.lower()\n",
    "    \n",
    "    # 3. Apply the rename\n",
    "    df_cleaning = df_cleaning.withColumnRenamed(old_name, new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2270f212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-----+-------+--------+--------------------+--------------+--------------+-----+----+------+--------+---------------------+\n",
      "|user|card|year|month| amount|use_chip|       merchant_name| merchant_city|merchant_state|  zip| mcc|errors|is_fraud|transaction_timestamp|\n",
      "+----+----+----+-----+-------+--------+--------------------+--------------+--------------+-----+----+------+--------+---------------------+\n",
      "| 591|   3|2016|    8|-415.00|    Chip| 4552887027432897467|       Oakland|            CA|94606|3596|   N/A|      No|  2016-08-13 10:54:00|\n",
      "| 591|   3|2016|    8|  22.37|    Chip|-8964802287130046767|        Tucker|            GA|30084|7230|   N/A|      No|  2016-08-16 13:33:00|\n",
      "| 591|   3|2016|    8|  10.87|    Chip|   97032797689821735|Southern Pines|            NC|28387|5411|   N/A|      No|  2016-08-19 14:38:00|\n",
      "| 591|   3|2016|    8|  73.84|    Chip|-5401953891366957779|       Shannon|            NC|28386|5651|   N/A|      No|  2016-08-20 10:11:00|\n",
      "| 591|   3|2016|    8|  38.50|    Chip|-2472481739355111587|   Saint Pauls|            NC|28384|7538|   N/A|      No|  2016-08-20 15:32:00|\n",
      "+----+----+----+-----+-------+--------+--------------------+--------------+--------------+-----+----+------+--------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cleaning.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e99ced03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user: integer (nullable = true)\n",
      " |-- card: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- amount: string (nullable = true)\n",
      " |-- use_chip: string (nullable = true)\n",
      " |-- merchant_name: string (nullable = true)\n",
      " |-- merchant_city: string (nullable = true)\n",
      " |-- merchant_state: string (nullable = true)\n",
      " |-- zip: string (nullable = false)\n",
      " |-- mcc: integer (nullable = true)\n",
      " |-- errors: string (nullable = false)\n",
      " |-- is_fraud: string (nullable = true)\n",
      " |-- transaction_timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cleaning.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf850dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Schema for Export:\n",
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- card_id: integer (nullable = true)\n",
      " |-- amount: decimal(10,2) (nullable = true)\n",
      " |-- use_chip: string (nullable = true)\n",
      " |-- merchant_name: string (nullable = true)\n",
      " |-- merchant_city: string (nullable = true)\n",
      " |-- merchant_state: string (nullable = true)\n",
      " |-- zip: string (nullable = false)\n",
      " |-- mcc: integer (nullable = true)\n",
      " |-- errors: string (nullable = false)\n",
      " |-- is_fraud: string (nullable = true)\n",
      " |-- transaction_timestamp: timestamp (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n",
      "Job Done !!!! Transactions table saved with correct column names.\n"
     ]
    }
   ],
   "source": [
    "df_final_prep = df_cleaning.withColumn(\n",
    "    \"amount\",\n",
    "    F.regexp_replace(F.col(\"amount\").cast(\"string\"), \"[^0-9\\\\.\\\\-]\", \"\").cast(DecimalType(10, 2))\n",
    ")\n",
    "\n",
    "# --- FIX 2: Explicit Column Selection & Renaming ---\n",
    "# We map 'user' -> 'user_id' and 'card' -> 'card_id' to match the Hive Table Schema.\n",
    "df_export = df_final_prep.select(\n",
    "    F.col(\"user\").alias(\"user_id\"),       # <--- CRITICAL FIX\n",
    "    F.col(\"card\").alias(\"card_id\"),       # <--- CRITICAL FIX\n",
    "    F.col(\"amount\"),\n",
    "    F.col(\"use_chip\"),\n",
    "    F.col(\"merchant_name\"),\n",
    "    F.col(\"merchant_city\"),\n",
    "    F.col(\"merchant_state\"),\n",
    "    F.col(\"zip\"),\n",
    "    F.col(\"mcc\"),\n",
    "    F.col(\"errors\"),\n",
    "    F.col(\"is_fraud\"),\n",
    "    F.col(\"transaction_timestamp\"),\n",
    "    F.col(\"year\"),   # Required for partitioning\n",
    "    F.col(\"month\")   # Required for partitioning\n",
    ")\n",
    "\n",
    "# Debug: Verify the schema matches Hive (user_id, card_id, etc.)\n",
    "print(\"Final Schema for Export:\")\n",
    "df_export.printSchema()\n",
    "\n",
    "# --- Save to Warehouse ---\n",
    "df_export.write.mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .format(\"orc\") \\\n",
    "    .save(\"/user/talentum/projectMaster/warehouseDir/transactions\")\n",
    "\n",
    "print('Job Done !!!! Transactions table saved with correct column names.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcb87b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- card_id: integer (nullable = true)\n",
      " |-- amount: decimal(10,2) (nullable = true)\n",
      " |-- use_chip: string (nullable = true)\n",
      " |-- merchant_name: string (nullable = true)\n",
      " |-- merchant_city: string (nullable = true)\n",
      " |-- merchant_state: string (nullable = true)\n",
      " |-- zip: string (nullable = true)\n",
      " |-- mcc: integer (nullable = true)\n",
      " |-- errors: string (nullable = true)\n",
      " |-- is_fraud: string (nullable = true)\n",
      " |-- transaction_timestamp: timestamp (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_df = spark.read.orc(\"/user/talentum/projectMaster/warehouseDir/transactions\")\n",
    "check_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca04e7fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
