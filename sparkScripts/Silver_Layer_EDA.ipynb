{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fc04443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12414d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Silver Layer EDA\").config(\"spark.driver.memory\", \"8g\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32bc9b08",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-58ba9b81883d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee568498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data location successfully retrieved: hdfs://localhost:9000/user/talentum/projectMaster/warehouseDir/transactions\n"
     ]
    }
   ],
   "source": [
    "# Execute SQL command to describe the table extended\n",
    "df_location_info = spark.sql(\"DESCRIBE EXTENDED financial_db.transactions_silver\")\n",
    "\n",
    "# Filter the result to find the row where 'col_name' is 'Location'\n",
    "# The actual file path is stored in the 'data_type' column for that specific metadata row.\n",
    "data_path = (\n",
    "    df_location_info.filter(df_location_info.col_name == \"Location\")\n",
    "    .select(\"data_type\")\n",
    "    .collect()[0][0]  # .collect() fetches data to driver, [0][0] extracts the single string value\n",
    ")\n",
    "\n",
    "print(f\"Data location successfully retrieved: {data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "488f4ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DecimalType, TimestampType, BooleanType\n",
    "\n",
    "# Define the explicit schema\n",
    "custom_schema = StructType([\n",
    "    StructField(\"user\", IntegerType(), True),\n",
    "    StructField(\"card\", IntegerType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"month\", IntegerType(), True),\n",
    "    StructField(\"amount\", StringType(), True),\n",
    "    StructField(\"use_chip\", StringType(), True),\n",
    "    StructField(\"merchant_name\", StringType(), True),\n",
    "    StructField(\"merchant_city\", StringType(), True),\n",
    "    StructField(\"merchant_state\", StringType(), True),\n",
    "    StructField(\"zip\", StringType(), True),\n",
    "    StructField(\"mcc\", IntegerType(), True),\n",
    "    StructField(\"errors\", StringType(), True),\n",
    "    StructField(\"is_fraud\", StringType(), True),\n",
    "    StructField(\"transaction_timestamp\", TimestampType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14a2f2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Schema applied successfully:\n",
      "root\n",
      " |-- user: integer (nullable = true)\n",
      " |-- card: integer (nullable = true)\n",
      " |-- amount: string (nullable = true)\n",
      " |-- use_chip: string (nullable = true)\n",
      " |-- merchant_name: string (nullable = true)\n",
      " |-- merchant_city: string (nullable = true)\n",
      " |-- merchant_state: string (nullable = true)\n",
      " |-- zip: string (nullable = true)\n",
      " |-- mcc: integer (nullable = true)\n",
      " |-- errors: string (nullable = true)\n",
      " |-- is_fraud: string (nullable = true)\n",
      " |-- transaction_timestamp: timestamp (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n",
      "Total Transactions: 24386900\n"
     ]
    }
   ],
   "source": [
    "# Load Tables\n",
    "# Read the data directly from the path, forcing the custom schema\n",
    "df_trans = spark.read \\\n",
    "    .schema(custom_schema) \\\n",
    "    .orc(data_path)\n",
    "\n",
    "print(\"\\nSchema applied successfully:\")\n",
    "df_trans.printSchema()\n",
    "\n",
    "print(f\"Total Transactions: {df_trans.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fae008ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+----------+\n",
      "|user|     person_id|birth_year|\n",
      "+----+--------------+----------+\n",
      "|   0|Hazel Robinson|      1966|\n",
      "|   1|    Sasha Sadr|      1966|\n",
      "|   2|    Saanvi Lee|      1938|\n",
      "|   3| Everlee Clark|      1957|\n",
      "|   4| Kyle Peterson|      1976|\n",
      "+----+--------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- person_id: string (nullable = true)\n",
      " |-- current_age: integer (nullable = true)\n",
      " |-- retirement_age: integer (nullable = true)\n",
      " |-- birth_year: integer (nullable = true)\n",
      " |-- birth_month: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- apartment: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- zipcode: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- per_capita_income_zipcode: decimal(10,2) (nullable = true)\n",
      " |-- yearly_income_person: decimal(10,2) (nullable = true)\n",
      " |-- total_debt: decimal(10,2) (nullable = true)\n",
      " |-- fico_score: integer (nullable = true)\n",
      " |-- num_credit_cards: integer (nullable = true)\n",
      " |-- user: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, year, row_number, monotonically_increasing_id\n",
    "\n",
    "df_users = spark.table(\"financial_db.users_silver\")\n",
    "window_spec = Window.orderBy(monotonically_increasing_id())\n",
    "\n",
    "# Add the 'person_id_int' column, starting the row number from 1, and cast to Integer\n",
    "df_users = df_users.withColumn(\n",
    "    \"user\", \n",
    "    (row_number().over(window_spec) -1).cast(IntegerType())\n",
    ")\n",
    "\n",
    "# Optional check: show the new ID column\n",
    "df_users.select(\"user\", \"person_id\", \"birth_year\").show(5)\n",
    "df_users.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1008801",
   "metadata": {},
   "source": [
    "## 1. Data Health & Integrity Check\n",
    "**Objective:** Validate that the Silver Layer cleaning logic (ETL) was successful before we start analysis.\n",
    "\n",
    "**Analysis Outcomes:**\n",
    "* **Null Checks:** We confirm that critical columns like `amount`, `zip`, and `transaction_timestamp` have **0 Nulls**, proving our `coalesce` and imputation strategies worked.\n",
    "* **Cardinality:**\n",
    "    * **Users:** ~2,000 unique users (matches the source dataset).\n",
    "    * **Merchants:** ~100k+ unique merchants, indicating a high-variety dataset.\n",
    "* **Class Imbalance:** We observe that Fraud cases (`is_fraud='Yes'`) make up only **~0.12%** of the dataset. This extreme imbalance confirms that accuracy will be a poor metric for our ML models later, and we must use metrics like **Recall** or **AUPRC**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "215cf0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Health Check ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'F' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5ac8e0300851>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 1. Null Count per Column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misNull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 2. Distinct Counts (Cardinality)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-5ac8e0300851>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 1. Null Count per Column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misNull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 2. Distinct Counts (Cardinality)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'F' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"--- Data Health Check ---\")\n",
    "\n",
    "# 1. Null Count per Column\n",
    "df_trans.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df_trans.columns]).show()\n",
    "\n",
    "# 2. Distinct Counts (Cardinality)\n",
    "df_trans.select(\n",
    "    F.countDistinct(\"user\").alias(\"Unique Users\"),\n",
    "    F.countDistinct(\"merchant_name\").alias(\"Unique Merchants\"),\n",
    "    F.countDistinct(\"card\").alias(\"Unique Cards\")\n",
    ").show()\n",
    "\n",
    "# 3. Class Imbalance (Crucial for Fraud Model)\n",
    "print(\"Fraud Class Distribution:\")\n",
    "df_trans.groupBy(\"is_fraud\").count().withColumn(\n",
    "    \"percentage\", F.col(\"count\") / df_trans.count() * 100\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7dcb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 5% of data, using 'is_fraud' to stratify if needed, \n",
    "# but random sampling is usually fine for distributions at this scale.\n",
    "sample_fraction = 0.05\n",
    "df_sample = df_trans.sample(withReplacement=False, fraction=sample_fraction, seed=42)\n",
    "\n",
    "# Join with Users to get Demographics for the sample\n",
    "df_sample_enriched = df_sample.join(df_users, df_sample[\"user\"] == df_users[\"user\"], \"left\")\n",
    "\n",
    "# Convert to Pandas\n",
    "# 1. Select the necessary columns and explicitly cast them to simple, safe types (float or string)\n",
    "df_safe = df_sample_enriched.select(\n",
    "    F.col(\"current_age\").cast(\"float\").alias(\"current_age\"),\n",
    "    F.col(\"amount\").cast(\"float\").alias(\"amount\"),\n",
    "    F.col(\"gender\").cast(\"string\").alias(\"gender\"),\n",
    "    F.col(\"transaction_timestamp\").cast(\"string\").alias(\"transaction_timestamp\"),\n",
    "    F.col(\"is_fraud\").cast(\"string\").alias(\"is_fraud\"),\n",
    "    F.col(\"yearly_income_person\").cast(\"float\").alias(\"yearly_income_person\")\n",
    ")\n",
    "\n",
    "# 2. Convert the SAFE DataFrame to Pandas\n",
    "# This should now bypass the internal PySpark/Pandas bug.\n",
    "pdf = df_safe.toPandas()\n",
    "\n",
    "# Fix types for plotting\n",
    "pdf['amount'] = pdf['amount'].astype(float)\n",
    "pdf['transaction_timestamp'] = pd.to_datetime(pdf['transaction_timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e75543",
   "metadata": {},
   "source": [
    "## 2. Univariate Analysis: Transaction Amount Distribution\n",
    "**Objective:** Understand the distribution of money spent to detect skewness and outliers.\n",
    "\n",
    "**Analysis Outcomes:**\n",
    "* **Log-Normal Distribution:** The plot (on a Log Scale) reveals a classic \"Bell Curve,\" meaning the underlying data follows a Log-Normal distribution.\n",
    "* **Implication for ML:** Since the raw data spans from $1 to $20,000+ with a heavy right skew, we **cannot** feed raw `Amount` into linear models (like Logistic Regression). We must apply a **Log Transformation** or **Standard Scaling** during the Feature Engineering phase.\n",
    "* **Negative Values:** Any negative values (refunds) were excluded from this specific log-plot to prevent errors, but they represent valid \"credit\" transactions in the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d1858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_positive = pdf[pdf['amount'] > 0]\n",
    "\n",
    "# 2. Plot\n",
    "plt.figure(figsize=(18, 9))\n",
    "sns.histplot(pdf_positive['amount'], bins=50, kde=True, log_scale=True)\n",
    "plt.title('Distribution of Transaction Amounts (Positive Only - Log Scale)')\n",
    "plt.xlabel('Amount ($)')\n",
    "plt.show()\n",
    "\n",
    "# 3.Check what we excluded\n",
    "non_positive_count = len(pdf) - len(pdf_positive)\n",
    "print(f\"Excluded {non_positive_count} transactions (Zero or Negative amounts) from the plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b133528b",
   "metadata": {},
   "source": [
    "## 3. Bivariate Analysis: Does Fraud cost more?\n",
    "**Objective:** Determine if fraudulent transactions tend to have higher dollar values than legitimate ones.\n",
    "\n",
    "**Analysis Outcomes:**\n",
    "* **Visual Evidence:** The boxplot shows the median transaction amount for **Fraud (Yes)** is often higher than **Legitimate (No)** transactions.\n",
    "* **Variance:** Fraudulent transactions show a wider spread in the upper quartiles, indicating fraudsters often attempt to maximize value before being caught.\n",
    "* **Feature Importance:** This confirms that `Amount` will be a **strong predictive feature** for our Fraud Detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37365ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 9))\n",
    "sns.boxplot(x='is_fraud', y='amount', data=pdf)\n",
    "plt.yscale('log') # Log scale handles the outliers better\n",
    "plt.title('Transaction Amount by Fraud Status')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aea5f8",
   "metadata": {},
   "source": [
    "## 4. Temporal Analysis: Peak Load Times\n",
    "**Objective:** Identify high-traffic windows for the Infrastructure Load Prediction model.\n",
    "\n",
    "**Analysis Outcomes:**\n",
    "* **Daily Cycles:** We observe clear \"waking hours\" activity (rising at 8 AM, peaking at 1 PM - 6 PM, dropping after 10 PM).\n",
    "* **The \"Sleeping\" Gap:** There is a distinct low-traffic zone between **2 AM and 5 AM**.\n",
    "* **Implication:**\n",
    "    * **For Load Prediction:** The model must heavily weigh `Hour_of_Day`.\n",
    "    * **For Fraud:** Any spike in volume during the \"Sleeping Gap\" (2 AM - 5 AM) is anomalous and highly suspicious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd7a77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 18))\n",
    "pdf['hour'] = pdf['transaction_timestamp'].dt.hour\n",
    "pdf['day_of_week'] = pdf['transaction_timestamp'].dt.day_name()\n",
    "\n",
    "# Pivot for Heatmap\n",
    "heatmap_data = pdf.pivot_table(index='day_of_week', columns='hour', values='amount', aggfunc='count')\n",
    "days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "heatmap_data = heatmap_data.reindex(days_order)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(heatmap_data, cmap=\"YlGnBu\")\n",
    "plt.title('Transaction Volume Heatmap (Day vs. Hour)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7431c927",
   "metadata": {},
   "source": [
    "## 5. Demographic Analysis: Spending Power by Age\n",
    "**Objective:** Validate User Profile data for the Spend Analysis Data Mart.\n",
    "\n",
    "**Analysis Outcomes:**\n",
    "* **Uniformity:** We likely see that spending amounts are relatively uniform across age groups, though \"Working Age\" adults (25-60) may have a higher density of high-value transactions compared to retirees (>65) or students (<22).\n",
    "* **Outliers:** This plot helps identifying data errors (e.g., if we saw an age of 150 or 0).\n",
    "* **Cluster Potential:** The visible density clusters suggest that `Age` combined with `Amount` can help segment users into groups like \"Budget Students\" vs. \"Wealthy Retirees\" in our Gold Layer clustering model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74f1d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_cleaned = pdf.dropna(subset=['current_age', 'gender', 'amount']).copy()\n",
    "pdf_positive = pdf_cleaned[pdf_cleaned['amount'] > 0].copy()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(\n",
    "    x='current_age', \n",
    "    y='amount', \n",
    "    hue='gender', \n",
    "    data=pdf_positive, \n",
    "    alpha=0.3\n",
    ")\n",
    "plt.title('Spending Amount vs. Age (by Gender)')\n",
    "plt.ylabel('Amount (Log Scale)') \n",
    "plt.xlabel('Current Age (Years)') \n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d6649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the descriptive statistics of the scaled age column\n",
    "print(\"Scaled Age Statistics:\")\n",
    "print(pdf_cleaned['current_age'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbd34ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd849e15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
