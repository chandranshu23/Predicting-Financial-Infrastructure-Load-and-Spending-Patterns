{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76320b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2a1b4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"create_merchant_recommendation_dataMart\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "426a5b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Config Updated for maximum stability.\n"
     ]
    }
   ],
   "source": [
    "# 1. Disable Vectorized Reader (Avoids low-level ORC data reading crash)\n",
    "spark.conf.set(\"spark.sql.orc.enableVectorizedReader\", \"false\")\n",
    "spark.conf.set(\"spark.sql.hive.convertMetastoreOrc\", \"false\")\n",
    "# 2. Disable Broadcast Join (Avoids memory/shuffle crash on join)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1) \n",
    "# 3. Disable Spark's optimizing components (Forces safer execution path)\n",
    "spark.conf.set(\"spark.sql.cbo.enabled\", \"false\") \n",
    "spark.conf.set(\"spark.sql.codegen.wholeStage\", \"false\")\n",
    "# 4. Force Hive SerDe (Ultimate attempt to bypass native Spark reader)\n",
    "spark.conf.set(\"spark.sql.hive.convertMetastore\", \"false\") \n",
    "\n",
    "print(\"Spark Config Updated for maximum stability.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe83aefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import DecimalType, LongType, IntegerType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2a7fa1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Silver Layer tables from Hive...\n",
      "Loading command executed\n"
     ]
    }
   ],
   "source": [
    "# Load the Tables directly from the Database\n",
    "# We use the standard 'database_name.table_name' format\n",
    "\n",
    "print(\"Loading Silver Layer tables from Hive...\")\n",
    "\n",
    "# A. Transactions Table\n",
    "df_trans_silver = spark.table(\"financial_db.transactions_silver\")\n",
    "\n",
    "# B. Users Table\n",
    "df_users_silver = spark.table(\"financial_db.users_silver\")\n",
    "\n",
    "# C. Cards Table\n",
    "df_cards_silver = spark.table(\"financial_db.cards_silver\")\n",
    "\n",
    "print(\"Loading command executed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a90cd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transactions Count: 24386900\n",
      "Users Count: 2000\n",
      "Cards Count: 6146\n"
     ]
    }
   ],
   "source": [
    "# run only if you are not sure the files are loaded\n",
    "# Quick Verification\n",
    "print(f\"Transactions Count: {df_trans_silver.count()}\")\n",
    "print(f\"Users Count: {df_users_silver.count()}\")\n",
    "print(f\"Cards Count: {df_cards_silver.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76d8a0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- card_id: integer (nullable = true)\n",
      " |-- amount: decimal(10,2) (nullable = true)\n",
      " |-- use_chip: string (nullable = true)\n",
      " |-- merchant_name: string (nullable = true)\n",
      " |-- merchant_city: string (nullable = true)\n",
      " |-- merchant_state: string (nullable = true)\n",
      " |-- zip: string (nullable = true)\n",
      " |-- mcc: integer (nullable = true)\n",
      " |-- errors: string (nullable = true)\n",
      " |-- is_fraud: string (nullable = true)\n",
      " |-- transaction_timestamp: timestamp (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- person_id: string (nullable = true)\n",
      " |-- current_age: integer (nullable = true)\n",
      " |-- retirement_age: integer (nullable = true)\n",
      " |-- birth_year: integer (nullable = true)\n",
      " |-- birth_month: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- apartment: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- zipcode: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- per_capita_income_zipcode: decimal(10,2) (nullable = true)\n",
      " |-- yearly_income_person: decimal(10,2) (nullable = true)\n",
      " |-- total_debt: decimal(10,2) (nullable = true)\n",
      " |-- fico_score: integer (nullable = true)\n",
      " |-- num_credit_cards: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- user: integer (nullable = true)\n",
      " |-- card_index: integer (nullable = true)\n",
      " |-- card_brand: string (nullable = true)\n",
      " |-- card_type: string (nullable = true)\n",
      " |-- card_number: integer (nullable = true)\n",
      " |-- expires: string (nullable = true)\n",
      " |-- cvv: integer (nullable = true)\n",
      " |-- has_chip: string (nullable = true)\n",
      " |-- cards_issued: integer (nullable = true)\n",
      " |-- credit_limit: decimal(10,2) (nullable = true)\n",
      " |-- year_pin_last_changed: integer (nullable = true)\n",
      " |-- card_on_dark_web: string (nullable = true)\n",
      " |-- acct_opened_month: integer (nullable = true)\n",
      " |-- acct_opened_year: integer (nullable = true)\n",
      " |-- expires_month: integer (nullable = true)\n",
      " |-- expires_year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Previewing Schema to ensure types are correct\n",
    "df_trans_silver.printSchema()\n",
    "df_users_silver.printSchema()\n",
    "df_cards_silver.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e54b48e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Renaming and Preparing DataFrames ---\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. Renaming & Preparation (Golden Layer Prep)\n",
    "# ==========================================\n",
    "print(\"--- Step 1: Renaming and Preparing DataFrames ---\")\n",
    "\n",
    "# --- A. Transactions Prep ---\n",
    "# Already has snake_case, but ensuring we have the time components needed\n",
    "df_trans_renamed = df_trans_silver.withColumnRenamed(\"zip\", \"merchant_zip\") \\\n",
    "    .withColumnRenamed(\"errors\", \"transaction_errors\") \\\n",
    "    .withColumn(\"day\", F.dayofmonth(F.col(\"transaction_timestamp\"))) \\\n",
    "    .withColumn(\"time\", F.date_format(F.col(\"transaction_timestamp\"), \"HH:mm:ss\"))\n",
    "\n",
    "# --- B. Users Prep ---\n",
    "# 1. Generate 'user_id' via Indexing (Crucial for joining)\n",
    "rdd_with_id = df_users_silver.rdd.zipWithIndex().map(lambda x: (x[1],) + tuple(x[0]))\n",
    "user_cols = [\"user_id\"] + df_users_silver.columns\n",
    "df_users_indexed = spark.createDataFrame(rdd_with_id, user_cols)\n",
    "\n",
    "# 2. Rename columns to your convenience\n",
    "df_users_renamed = df_users_indexed \\\n",
    "    .withColumnRenamed(\"current_age\", \"age\") \\\n",
    "    .withColumnRenamed(\"city\", \"user_city\") \\\n",
    "    .withColumnRenamed(\"state\", \"user_state\") \\\n",
    "    .withColumnRenamed(\"per_capita_income_zipcode\", \"per_capita_income\") \\\n",
    "    .withColumnRenamed(\"yearly_income_person\", \"yearly_income\") \\\n",
    "    .withColumnRenamed(\"total_debt\", \"total_debt\")\n",
    "\n",
    "# --- C. Cards Prep ---\n",
    "# Rename keys to match Transactions and create Date for Age Calculation\n",
    "df_cards_renamed = df_cards_silver \\\n",
    "    .withColumnRenamed(\"user\", \"user_id\") \\\n",
    "    .withColumnRenamed(\"card_index\", \"card_id\") \\\n",
    "    .withColumnRenamed(\"card_on_dark_web\", \"dark_web_exposure\") \\\n",
    "    .withColumnRenamed(\"acct_opened_year\", \"acct_year\") \\\n",
    "    .withColumnRenamed(\"acct_opened_month\", \"acct_month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "29d960b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 2: Joining Transactions with Users ---\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 2. Join Transactions with Users\n",
    "# ==========================================\n",
    "print(\"--- Step 2: Joining Transactions with Users ---\")\n",
    "\n",
    "df_join_users = df_trans_renamed.join(\n",
    "    df_users_renamed,\n",
    "    on=\"user_id\",\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c439cb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 3: Selecting All Required Columns ---\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 3. First Selection (Intermediate DataFrame)\n",
    "# ==========================================\n",
    "print(\"--- Step 3: Selecting All Required Columns ---\")\n",
    "\n",
    "df_step3 = df_join_users.select(\n",
    "    # --- Keys ---\n",
    "    F.col(\"user_id\"),\n",
    "    F.col(\"card_id\"),\n",
    "    \n",
    "    # --- Transaction Data (CRITICAL: Don't drop these!) ---\n",
    "    F.col(\"amount\"),\n",
    "    F.col(\"transaction_timestamp\"),\n",
    "    F.col(\"year\"),\n",
    "    F.col(\"month\"),\n",
    "    F.col(\"day\"),\n",
    "    F.col(\"time\"),\n",
    "    F.col(\"merchant_state\"),        # Needed for Traveller Flag\n",
    "    F.col(\"transaction_errors\"),    # Needed for Health Metrics\n",
    "    F.col(\"is_fraud\"),              # Needed for Health Metrics\n",
    "    F.col(\"use_chip\"),              # Needed for Tech Complexity\n",
    "    \n",
    "    # --- User Demographics ---\n",
    "    F.col(\"age\"),\n",
    "    F.col(\"gender\"),\n",
    "    F.col(\"user_city\"),\n",
    "    F.col(\"user_state\"),            # Keeping original name to avoid confusion\n",
    "    F.col(\"per_capita_income\"),\n",
    "    F.col(\"yearly_income\"),\n",
    "    F.col(\"fico_score\"),\n",
    "    F.col(\"num_credit_cards\"),\n",
    "    F.col(\"total_debt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "35e43957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 4: Joining with Cards ---\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 4. Join with Cards\n",
    "# ==========================================\n",
    "print(\"--- Step 4: Joining with Cards ---\")\n",
    "\n",
    "df_join_cards = df_step3.join(\n",
    "    df_cards_renamed,\n",
    "    on=[\"user_id\", \"card_id\"],\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81f2dbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 5: Advanced Feature Engineering ---\n",
      "Features Created Successfully.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 5. Feature Engineering\n",
    "# ==========================================\n",
    "print(\"--- Step 5: Advanced Feature Engineering ---\")\n",
    "\n",
    "df_enriched = df_join_cards.withColumn(\n",
    "    # A. Age of Card (Derived)\n",
    "    \"age_of_card_years\",\n",
    "    F.col(\"year\") - F.col(\"acct_year\")\n",
    ").withColumn(\n",
    "    # B. Financial Ratios\n",
    "    \"debt_to_income_ratio\",\n",
    "    when(col(\"yearly_income\") > 0, \n",
    "         col(\"total_debt\") / col(\"yearly_income\")\n",
    "    ).otherwise(0.0)\n",
    ").withColumn(\n",
    "    \"credit_power\",\n",
    "    when(col(\"yearly_income\") > 0, \n",
    "         col(\"credit_limit\") / col(\"yearly_income\")\n",
    "    ).otherwise(0.0)\n",
    ").withColumn(\n",
    "    # C. Life Stage\n",
    "    \"life_stage\",\n",
    "    when(col(\"age\") < 30, \"Young_Adult\")\n",
    "    .when((col(\"age\") >= 30) & (col(\"age\") < 60), \"Working_Professional\")\n",
    "    .otherwise(\"Retired\")\n",
    ").withColumn(\n",
    "    # D. Traveller Flag (FIXED: Using correct column names)\n",
    "    \"is_travelling\",\n",
    "    when(\n",
    "        (col(\"merchant_state\").isNotNull()) & \n",
    "        (col(\"user_state\").isNotNull()) & \n",
    "        (col(\"merchant_state\") != col(\"user_state\")), \n",
    "        1\n",
    "    ).otherwise(0)\n",
    ").withColumn(\n",
    "    # E. Tech Complexity\n",
    "    \"tech_type\",\n",
    "    when(col(\"use_chip\").like(\"%Chip%\"), \"Chip\")\n",
    "    .when(col(\"use_chip\").like(\"%Online%\"), \"Online\")\n",
    "    .otherwise(\"Swipe\")\n",
    ")\n",
    "\n",
    "print(\"Features Created Successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cf52dd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 6: Aggregating into 15-Minute Windows ---\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 6. Aggregation (Gold Layer Data Mart)\n",
    "# ==========================================\n",
    "print(\"--- Step 6: Aggregating into 15-Minute Windows ---\")\n",
    "\n",
    "df_gold_mart = df_enriched.groupBy(\n",
    "    window(col(\"transaction_timestamp\"), \"15 minutes\")\n",
    ").agg(\n",
    "    # --- Targets ---\n",
    "    count(\"*\").alias(\"transaction_count\"),\n",
    "    sum(\"amount\").alias(\"total_volume\"),\n",
    "    avg(\"amount\").alias(\"avg_transaction_size\"),\n",
    "    \n",
    "    # --- Health ---\n",
    "    count(when(col(\"transaction_errors\") != \"N/A\", 1)).alias(\"error_count\"),\n",
    "    count(when(col(\"is_fraud\") == \"Yes\", 1)).alias(\"fraud_count\"),\n",
    "    \n",
    "    # --- Features (Averaged/Summed for the Time Window) ---\n",
    "    avg(\"debt_to_income_ratio\").alias(\"avg_debt_ratio\"),\n",
    "    avg(\"credit_power\").alias(\"avg_credit_power\"),\n",
    "    sum(\"is_travelling\").alias(\"traveller_count\"),\n",
    "    \n",
    "    # --- Demographics Breakdown ---\n",
    "    count(when(col(\"life_stage\") == \"Young_Adult\", 1)).alias(\"cnt_young_adult\"),\n",
    "    count(when(col(\"life_stage\") == \"Working_Professional\", 1)).alias(\"cnt_professional\"),\n",
    "    count(when(col(\"life_stage\") == \"Retired\", 1)).alias(\"cnt_retired\"),\n",
    "    \n",
    "    # --- Tech Breakdown ---\n",
    "    count(when(col(\"tech_type\") == \"Chip\", 1)).alias(\"cnt_chip\"),\n",
    "    count(when(col(\"tech_type\") == \"Online\", 1)).alias(\"cnt_online\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "15e342f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lit, window, avg, sum, count, countDistinct, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "18707620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 7: Final Preview ---\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 7. Final Formatting & Preview\n",
    "# ==========================================\n",
    "print(\"--- Step 7: Final Preview ---\")\n",
    "\n",
    "df_load_forecasting_gold = df_gold_mart.withColumn(\n",
    "    \"ds\", \n",
    "    col(\"window.start\")\n",
    ").drop(\"window\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0f9b75b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 6: Final Data Mart Preview ---\n",
      "root\n",
      " |-- transaction_count: long (nullable = false)\n",
      " |-- total_volume: decimal(20,2) (nullable = true)\n",
      " |-- avg_transaction_size: decimal(14,6) (nullable = true)\n",
      " |-- error_count: long (nullable = false)\n",
      " |-- fraud_count: long (nullable = false)\n",
      " |-- avg_debt_ratio: double (nullable = true)\n",
      " |-- avg_credit_power: double (nullable = true)\n",
      " |-- traveller_count: long (nullable = true)\n",
      " |-- cnt_young_adult: long (nullable = false)\n",
      " |-- cnt_professional: long (nullable = false)\n",
      " |-- cnt_retired: long (nullable = false)\n",
      " |-- cnt_chip: long (nullable = false)\n",
      " |-- cnt_online: long (nullable = false)\n",
      " |-- ds: timestamp (nullable = true)\n",
      "\n",
      "+-----------------+------------+--------------------+-----------+-----------+--------------+----------------+---------------+---------------+----------------+-----------+--------+----------+-------------------+\n",
      "|transaction_count|total_volume|avg_transaction_size|error_count|fraud_count|avg_debt_ratio|avg_credit_power|traveller_count|cnt_young_adult|cnt_professional|cnt_retired|cnt_chip|cnt_online|ds                 |\n",
      "+-----------------+------------+--------------------+-----------+-----------+--------------+----------------+---------------+---------------+----------------+-----------+--------+----------+-------------------+\n",
      "|1                |68.00       |68.000000           |0          |0          |0.611185      |0.467521721142  |0              |0              |1               |0          |0       |0         |1991-01-02 07:00:00|\n",
      "|2                |45.62       |22.810000           |0          |0          |0.611185      |0.467521721142  |0              |0              |2               |0          |0       |0         |1991-01-02 07:15:00|\n",
      "|1                |114.73      |114.730000          |0          |0          |0.611185      |0.467521721142  |0              |0              |1               |0          |0       |0         |1991-01-02 17:30:00|\n",
      "|1                |251.71      |251.710000          |0          |0          |0.611185      |0.467521721142  |0              |0              |1               |0          |0       |0         |1991-01-03 09:00:00|\n",
      "|1                |16.28       |16.280000           |0          |0          |0.611185      |0.467521721142  |0              |0              |1               |0          |0       |0         |1991-01-03 11:00:00|\n",
      "+-----------------+------------+--------------------+-----------+-----------+--------------+----------------+---------------+---------------+----------------+-----------+--------+----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 6. Preview Output\n",
    "# ==========================================\n",
    "print(\"--- Step 6: Final Data Mart Preview ---\")\n",
    "df_load_forecasting_gold.printSchema()\n",
    "df_load_forecasting_gold.orderBy(\"ds\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "40af90be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 6: Saving as External Hive Table ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Step 6: Saving as External Hive Table ---\")\n",
    "# Path for the External Table\n",
    "gold_path = \"/user/talentum/projectMaster/warehouseDir/gold/financial_load_forecasting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4a66c3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark.conf.set(\"spark.sql.orc.enableVectorizedReader\", \"false\")\n",
    "spark.conf.set(\"spark.sql.hive.convertMetastoreOrc\", \"false\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a6f865d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Table saved!\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS financial_db.load_forecasting_gold\")\n",
    "\n",
    "# Write with explicit partitioning and Hive serde override\n",
    "df_load_forecasting_gold.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"orc\") \\\n",
    "    .option(\"path\", \"/user/talentum/projectMaster/warehouseDir/gold/financial_laod_forecasting\") \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .saveAsTable(\"financial_db.load_forecasting_gold\")\n",
    "\n",
    "print(\"SUCCESS: Table saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7938ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_load_forecasting_gold.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae819a97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464fdf9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c142ba77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
